{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Basic Text Preprocessing"
      ],
      "metadata": {
        "id": "i0qmhAG9w2Ib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Toolkit (NLTK) is one of the largest Python libraries for performing various Natural Language Processing tasks. From rudimentary tasks such as text pre-processing to tasks like vectorized representation of text – NLTK’s API has covered everything. In this article, we will accustom ourselves to the basics of NLTK and perform some crucial NLP tasks: Tokenization, Stemming, Lemmatization, and POS Tagging."
      ],
      "metadata": {
        "id": "_s_MMnuxw8Ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural Language Toolkit (NLTK) works as a powerful Python library that a wide range of tools for Natural Language Processing (NLP). From fundamental tasks like text pre-processing to more advanced operations such as semantic reasoning, NLTK provides a versatile API that caters to the diverse needs of language-related tasks."
      ],
      "metadata": {
        "id": "jaYwJT0tuDnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "HV9xFaK8wr4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Accessing Additional Resources:\n",
        "To incorporate the usage of additional resources, such as recourses of languages other than English – you can run the following in a python script. It has to be done only once when you are running it for the first time in your system."
      ],
      "metadata": {
        "id": "q-NZ50qbwsKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "PPjyNeMquC-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenisation using NLTK**"
      ],
      "metadata": {
        "id": "JhKumQI4vQUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization refers to break down the text into smaller units. It entails splitting paragraphs into sentences and sentences into words. It is one of the initial steps of any NLP pipeline. Let us have a look at the two major kinds of tokenization that NLTK provides"
      ],
      "metadata": {
        "id": "UXev7huvxINe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document-> Pargraphs-> Sentences-> words\n",
        "\n",
        "##Sentence-level Tokenisation"
      ],
      "metadata": {
        "id": "Kr2-r_ZTo1wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization using NLTK\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "paragraph = \"NPTEL is a great learning platform. It is one of the best for Computer Science students.\"\n",
        "#print(word_tokenize(sent))\n",
        "print(sent_tokenize(paragraph))"
      ],
      "metadata": {
        "id": "2hVSe43duCLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenised_sentences=sent_tokenize(paragraph)\n",
        "print(len(tokenised_sentences))"
      ],
      "metadata": {
        "id": "GIoysDzVqh5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(tokenised_sentences))"
      ],
      "metadata": {
        "id": "VrtxLdqlrScV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenised_sentences[0])"
      ],
      "metadata": {
        "id": "s6Wqylnyq7cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenised_sentences[1])"
      ],
      "metadata": {
        "id": "LU6cXheirB_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(print(tokenised_sentences[0])))"
      ],
      "metadata": {
        "id": "IWujdEIFrFvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization using NLTK\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "paragraph1 = \"NPTEL is a great learning platform.It is one of the best for Computer Science students.\"\n",
        "#print(word_tokenize(sent))\n",
        "print(sent_tokenize(paragraph1))"
      ],
      "metadata": {
        "id": "RuDmfmlFsPe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** \"I study Machine Learning on GeeksforGeeks.\" will be word-tokenized as\n",
        "  ['I', 'study', 'Machine', 'Learning', 'on', 'GeeksforGeeks', '.']. **"
      ],
      "metadata": {
        "id": "3Lhh-Jiivu-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentence Tokenization\n",
        "It involves breaking down the text into individual sentences.\n",
        "\n",
        "Example:\n",
        "\"I study Machine Learning on GeeksforGeeks. Currently, I'm studying NLP\"\n",
        " will be sentence-tokenized as\n",
        " ['I study Machine Learning on GeeksforGeeks.', 'Currently, I'm studying NLP.']**"
      ],
      "metadata": {
        "id": "PWsmGWPtvykB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word-level Tokenisation"
      ],
      "metadata": {
        "id": "7pIWG0wQtsD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization using NLTK\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "#paragraph = \"NPTEL is a great learning platform. It is one of the best for Computer Science students.\"\n",
        "print(tokenised_sentences[0])\n",
        "#print(word_tokenize(tokenised_sentences[0]))"
      ],
      "metadata": {
        "id": "cu3DGAUiswL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(tokenised_sentences[0]))"
      ],
      "metadata": {
        "id": "tEMz2hj4tqih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(word_tokenize(tokenised_sentences[0])))"
      ],
      "metadata": {
        "id": "_cr0LGQft5xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenised_sentences[1])"
      ],
      "metadata": {
        "id": "P3V3bsjLuSE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(tokenised_sentences[1]))"
      ],
      "metadata": {
        "id": "HvliGlEQuRGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(word_tokenize(tokenised_sentences[1])))"
      ],
      "metadata": {
        "id": "n9wNHUIluY5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming using Porter Stemmer"
      ],
      "metadata": {
        "id": "Vo2ce8USvlvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "liking-> lik\n",
        "\n",
        "liked-> like/lik"
      ],
      "metadata": {
        "id": "SPTWeYOVv0oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming generates the base word from the inflected word by removing the affixes of the word. It has a set of pre-defined rules that govern the dropping of these affixes. It must be noted that stemmers might not always result in semantically meaningful base words.  Stemmers are faster and computationally less expensive than lemmatizers."
      ],
      "metadata": {
        "id": "GPiK784WwAtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that all the variations of the word ‘play’ have been reduced to the same word  – ‘play’. In this case, the output is a meaningful word, ‘play’. However, this is not always the case. Let us take an example.\n",
        "\n",
        "Please note that these groups are stored in the lemmatizer; there is no removal of affixes as in the case of a stemmer.\n"
      ],
      "metadata": {
        "id": "eXFYqMRawE2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "#Create an instance of porter stemmer\n",
        "porter_stemmer=PorterStemmer()\n",
        "original_words=['liking','likes','liked','likely']\n",
        "stemmed_words=[porter_stemmer.stem(word) for word in original_words]\n",
        "\n",
        "print(\"Original words\", original_words)\n",
        "print(\"Stemmed words using Porter Stemmer\",stemmed_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "U4-9DQB0wfqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "#Create an instance of porter stemmer\n",
        "porter_stemmer=PorterStemmer()\n",
        "original_words=['liking','likes','liked','likely']\n",
        "stemmed_words=[porter_stemmer.stem(word) for word in original_words]\n",
        "\n",
        "print(\"Original words\", original_words)\n",
        "print(\"Stemmed words using Porter Stemmer\",stemmed_words)"
      ],
      "metadata": {
        "id": "dIXJQtiuxv7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "#Create an instance of porter stemmer\n",
        "porter_stemmer=PorterStemmer()\n",
        "original_words=['plays','happily','ate','drank', 'running','go','goes','went']\n",
        "stemmed_words=[porter_stemmer.stem(word) for word in original_words]\n",
        "\n",
        "print(\"Original words\", original_words)\n",
        "print(\"Stemmed words using Porter Stemmer\",stemmed_words)"
      ],
      "metadata": {
        "id": "XdKRn_kYxwB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EED-> EE\n",
        "from nltk.stem import PorterStemmer\n",
        "#Create an instance of porter stemmer\n",
        "ps=PorterStemmer()\n",
        "original_words=['agreed']\n",
        "stemmed_words=[ps.stem(word) for word in original_words]\n",
        "\n",
        "print(\"Original words\", original_words)\n",
        "print(\"Stemmed words using Porter Stemmer\",stemmed_words)"
      ],
      "metadata": {
        "id": "EPhPFS0Fylvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "# create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "print(porter.stem(\"Communication\"))"
      ],
      "metadata": {
        "id": "ipp9Kh6NvZmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#EED-> EE\n",
        "from nltk.stem import PorterStemmer\n",
        "#Create an instance of porter stemmer\n",
        "ps=PorterStemmer()\n",
        "original_words=['Conclude', 'conclusive', 'conclusion']\n",
        "stemmed_words=[ps.stem(word) for word in original_words]\n",
        "\n",
        "print(\"Original words\", original_words)\n",
        "print(\"Stemmed words using Porter Stemmer\",stemmed_words)"
      ],
      "metadata": {
        "id": "dQS9lA4AzT1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#EED-> 0\n",
        "from nltk.stem import PorterStemmer\n",
        "#Create an instance of porter stemmer\n",
        "ps=PorterStemmer()\n",
        "original_words=['programming', 'elephant', 'mountain', 'sunshine', 'keyboard', 'ocean', 'umbrella', 'happiness', 'giraffe', 'whisper']\n",
        "stemmed_words=[ps.stem(word) for word in original_words]\n",
        "\n",
        "print(\"Original words\", original_words)\n",
        "print(\"Stemmed words using Porter Stemmer\",stemmed_words)"
      ],
      "metadata": {
        "id": "3urLzuA4zh2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization\n",
        "Lemmatization involves grouping together the inflected forms of the same word. This way, we can reach out to the base form of any word which will be meaningful in nature. The base from here is called the Lemma.\n",
        "\n",
        "Lemmatizers are slower and computationally more expensive than stemmers."
      ],
      "metadata": {
        "id": "ntNzvmhrwRQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "#Create an instance of porter stemmer\n",
        "porter_stemmer=PorterStemmer()\n",
        "original_words=['plays','happily','ate','drank', 'running','go','goes','went']\n",
        "stemmed_words=[porter_stemmer.stem(word) for word in original_words]\n",
        "\n",
        "print(\"Original words\", original_words)\n",
        "print(\"Stemmed words using Porter Stemmer\",stemmed_words)"
      ],
      "metadata": {
        "id": "aPxyJ8Lb0Rs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "#Create an instance of porter stemmer\n",
        "porter_stemmer=PorterStemmer()\n",
        "original_words1=['plays','happily','ate','drank', 'running','go','goes','went']\n",
        "stemmed_words1=[ ]\n",
        "for word in original_words1:\n",
        "  stemmed_words1.append(porter_stemmer.stem(word))\n",
        "print(\"Original words\", original_words1)\n",
        "print(\"Stemmed words using Porter Stemmer\",stemmed_words1)"
      ],
      "metadata": {
        "id": "2JU4_gbS3eoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# create an object of class WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# lemmatized_words=[lemmatizer.lemmatize(word) for word in original_words]\n",
        "# original_words=['plays','happily','ate','drank', 'running','go','goes','went']\n",
        "# print(lemmatized_words)\n",
        "print(lemmatizer.lemmatize(\"happily\",'v'))\n",
        "print(lemmatizer.lemmatize(\"goes\",'v'))\n",
        "print(lemmatizer.lemmatize(\"running\"))\n",
        "print(lemmatizer.lemmatize(\"running\",'v'))"
      ],
      "metadata": {
        "id": "0bmUd0rdwLTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize(\"the\",'dt'))"
      ],
      "metadata": {
        "id": "UtPGkxl54Q__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that in lemmatizers, we need to pass the Part of Speech of the word along with the word as a function argument.\n",
        "\n",
        "Also, stemmers always result in meaningful base words. Let us take the same example as we took in the case for stemmers."
      ],
      "metadata": {
        "id": "xPUm6bi3xQKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# create an object of class WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"Communication\", 'v'))"
      ],
      "metadata": {
        "id": "o19YKmrZxRU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why NLTK?\n",
        "\n",
        "**Popularity**: NLTK is one of the leading platforms for dealing with language data.\n",
        "\n",
        "**Simplicity**: Provides easy-to-use APIs for a wide variety of text preprocessing methods.\n",
        "\n",
        "**Community**: It has a large and active community that supports the library and improves it.\n",
        "\n",
        "**Open Source**: Free and open-source available for Windows, Mac OSX, and Linux."
      ],
      "metadata": {
        "id": "z2D1-Kzbxjh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Basic Preprocessing for Text"
      ],
      "metadata": {
        "id": "heMHwZlg3gfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lowercase**"
      ],
      "metadata": {
        "id": "ltqSS-jX3tJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\n",
        "text = text.lower()\n",
        "print(text)"
      ],
      "metadata": {
        "id": "YYF7fKUExiUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\n",
        "text = text.upper()\n",
        "print(text)"
      ],
      "metadata": {
        "id": "ISJyBRoF5Vg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Punctuation**"
      ],
      "metadata": {
        "id": "mzzQqrRoxi6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet=\"Great!@justinbeiber concert was amazing!!!!\\#neversaynever.\""
      ],
      "metadata": {
        "id": "fXfdszt55eas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "print(string.punctuation)"
      ],
      "metadata": {
        "id": "3XbSYeCC33ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_tweet = \"\".join([char for char in tweet if char not in string.punctuation])\n",
        "print(pre_tweet)"
      ],
      "metadata": {
        "id": "d_BWMM2F357V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"card-carrying\"\n",
        "print(len(query))\n",
        "pre_query = \"\".join([char for char in query if char not in string.punctuation])\n",
        "print(pre_query)\n",
        "print(len(pre_query))"
      ],
      "metadata": {
        "id": "FDAcPwqA6uX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function words\n",
        "\n",
        "Content words"
      ],
      "metadata": {
        "id": "GVONE9LB6lcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopword Filtering**"
      ],
      "metadata": {
        "id": "9V2f8GWT39RB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words_eng = stopwords.words('english')\n",
        "print(stop_words_eng)\n",
        "print(len(stop_words_eng))"
      ],
      "metadata": {
        "id": "MqxXLuXh371v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words_chinese = stopwords.words('chinese')\n",
        "print(stop_words_chinese)\n",
        "print(len(stop_words_chinese))"
      ],
      "metadata": {
        "id": "y4ovKGO78I56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words_eng = stopwords.words('english')\n",
        "words='I have enrolled in NLP course on NPTEL by Dr. Pawan Goyal'\n",
        "tokens=word_tokenize(words)"
      ],
      "metadata": {
        "id": "1r65Om4v4Ekl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "id": "_t_Zz93N86hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=[word for word in tokens if word.lower() not in stop_words_eng]"
      ],
      "metadata": {
        "id": "L5MWy-st_LPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "WgKVWB0t_XmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop word filetring\n",
        "\n",
        "Tokenise-> list comprehension"
      ],
      "metadata": {
        "id": "0gnTG23t_n9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(output))"
      ],
      "metadata": {
        "id": "_JymlFQK_ZJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out=\"\".join([str(ele)+\" \" for ele in output])"
      ],
      "metadata": {
        "id": "TI5uT_N-_wKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "id": "hwtW76nG_yeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0BI_HMf8_-os"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}